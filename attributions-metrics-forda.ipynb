{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba872b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3499a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schlegel/.conda/captum/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sktime.datasets import load_UCR_UEA_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d789f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exp_attributions as exp_att\n",
    "import exp_perturbation_analysis as exp_pa\n",
    "import exp_perturbation_card as exp_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25196c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 13\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'FordA'\n",
    "dataset_name = dataset.lower()\n",
    "\n",
    "cur_time = time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "base_dir = f'./results/{dataset_name}--{cur_time}'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1571bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_UCR_UEA_dataset(name=dataset, split='train', return_type='numpyflat')\n",
    "X_test, y_test = load_UCR_UEA_dataset(name=dataset, split='test', return_type='numpyflat')\n",
    "\n",
    "print(f'Length training data: {len(X_train)} labels: {len(y_train)} test data: {len(X_test)} labels: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd56d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schlegel/.conda/captum/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "\n",
    "y_train_ohe = encoder.fit_transform(np.expand_dims(y_train, axis=-1))\n",
    "y_test_ohe = encoder.transform(np.expand_dims(y_test, axis=-1))\n",
    "\n",
    "y_train_norm = y_train_ohe.argmax(axis=-1)\n",
    "y_test_norm = y_test_ohe.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50374c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FordADataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692ad4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = FordADataset(X_train, y_train_ohe)\n",
    "dataset_test = FordADataset(X_test, y_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36713b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=120, shuffle=True)\n",
    "dataloader_train_not_shuffled = DataLoader(dataset_train, batch_size=120, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=120, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e39b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 10, kernel_size=3, stride=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(10, 50, kernel_size=3, stride=1),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(50, 100, kernel_size=3, stride=1),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(100 * 54, 100),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(100, 2),\n",
    "            nn.Softmax(-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af59d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, dataloader_train, criterion):\n",
    "    running_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader_train):\n",
    "        inputs = inputs.reshape(inputs.shape[0], 1, -1)\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, labels.argmax(dim=-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(dataloader_train)\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validator(model, dataloader_test, criterion):\n",
    "    running_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader_test):\n",
    "        inputs = inputs.reshape(inputs.shape[0], 1, -1)\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, labels.argmax(dim=-1))\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(dataloader_train)\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b1a4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f27a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val 0.24538052466607863\n",
      "Val 0.2271355063684525\n",
      "Val 0.19523285473546675\n",
      "Val 0.1737615898732216\n",
      "Val 0.16192963334821886\n",
      "Val 0.15819191932678223\n",
      "Val 0.15484415235057955\n",
      "Val 0.15367361518644518\n",
      "Val 0.150385546107446\n",
      "Val 0.15013589974372618\n",
      "Val 0.14899024559605506\n",
      "Val 0.14794734120368958\n",
      "Val 0.1470267628469775\n",
      "Val 0.14894994131980405\n",
      "Val 0.14720758411192125\n",
      "Val 0.14759679763547837\n",
      "Val 0.14933574968768704\n",
      "Val 0.14568548625515354\n",
      "Val 0.14541851897393504\n",
      "Val 0.14596126637151163\n",
      "Val 0.14974524032685063\n",
      "Val 0.14596286319917248\n",
      "Val 0.1468512983091416\n",
      "Val 0.14665600753599597\n",
      "Val 0.14690742954131095\n",
      "Val 0.14606425646812685\n",
      "Val 0.14628386593634082\n",
      "Val 0.1456132761893734\n",
      "Val 0.1460236003321986\n",
      "Val 0.14587083843446547\n",
      "Val 0.146202600771381\n",
      "Val 0.14619139317543275\n",
      "Val 0.14556700760318386\n",
      "Val 0.14618127961312571\n",
      "Val 0.1455558269254623\n",
      "Val 0.14657215918264083\n",
      "Val 0.14776922618189164\n",
      "Val 0.1462692810643104\n",
      "Val 0.14749211938150467\n",
      "Val 0.1466430233370873\n",
      "Val 0.15436521845479165\n",
      "Val 0.14800879166972253\n",
      "Val 0.14831933090763708\n",
      "Val 0.14880804957882052\n",
      "Val 0.14629304889709718\n",
      "Val 0.14661924877474386\n",
      "Val 0.14688439523020097\n",
      "Val 0.14645452460935038\n",
      "Val 0.14663013431333727\n",
      "Val 0.14706931960198186\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = trainer(model, dataloader_train, loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Val', validator(model, dataloader_test, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0186bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy Train 0.9936\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "for x in dataloader_train_not_shuffled:\n",
    "    input_, label_ = x\n",
    "    input_ = input_.reshape(input_.shape[0], 1, -1)\n",
    "    input_ = input_.float().to(device)\n",
    "    label_ = label_.float().to(device)\n",
    "\n",
    "    pred_ = model(input_)\n",
    "    preds.extend(pred_)\n",
    "    labels.extend(label_)\n",
    "\n",
    "preds = torch.stack(preds)\n",
    "labels = torch.stack(labels)\n",
    "print('Prediction Accuracy Train', np.round((preds.argmax(dim=-1) == labels.argmax(dim=-1)).int().sum().float().item() / len(preds), 4))\n",
    "\n",
    "y_train_pred = preds.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46a22b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy Test 0.8894\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "for x in dataloader_test:\n",
    "    input_, label_ = x\n",
    "    input_ = input_.reshape(input_.shape[0], 1, -1)\n",
    "    input_ = input_.float().to(device)\n",
    "    label_ = label_.float().to(device)\n",
    "\n",
    "    pred_ = model(input_)\n",
    "    preds.extend(pred_)\n",
    "    labels.extend(label_)\n",
    "\n",
    "preds = torch.stack(preds)\n",
    "labels = torch.stack(labels)\n",
    "print('Prediction Accuracy Test', np.round((preds.argmax(dim=-1) == labels.argmax(dim=-1)).int().sum().float().item() / len(preds), 4))\n",
    "\n",
    "y_test_pred = preds.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560a9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07461c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec084e2e",
   "metadata": {},
   "source": [
    "# Generate attributions for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901cae2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(1, 10, kernel_size=(3,), stride=(1,))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv1d(10, 50, kernel_size=(3,), stride=(1,))\n",
       "    (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv1d(50, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=5400, out_features=100, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239688e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, label = dataset_train[0]\n",
    "shape = sample.reshape(1, -1).shape\n",
    "baselines = torch.from_numpy(np.array([dataset_train[torch.randint(len(dataset_train), (1,))][0] for _ in range(10)])).reshape(-1, *shape).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8958bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start with DeepLiftShap             :   0%|          | 0/31 [00:00<?, ?it/s]/home/schlegel/.conda/captum/lib/python3.10/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n",
      "Start with DeepLiftShap             : 100%|██████████| 31/31 [00:03<00:00,  9.76it/s]\n",
      "Start with GradientShap             : 100%|██████████| 31/31 [00:00<00:00, 50.38it/s]\n",
      "Start with IntegratedGradients      : 100%|██████████| 31/31 [00:05<00:00,  5.21it/s]\n",
      "Start with Saliency                 :   0%|          | 0/31 [00:00<?, ?it/s]/home/schlegel/.conda/captum/lib/python3.10/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "Start with Saliency                 : 100%|██████████| 31/31 [00:00<00:00, 75.80it/s]\n",
      "Start with DeepLift                 : 100%|██████████| 31/31 [00:00<00:00, 62.01it/s]\n",
      "Start with Occlusion                : 100%|██████████| 31/31 [00:20<00:00,  1.48it/s]\n",
      "Start with KernelShap               : 100%|██████████| 31/31 [06:20<00:00, 12.29s/it]\n"
     ]
    }
   ],
   "source": [
    "attributions_train = {}\n",
    "predictions_train = {}\n",
    "\n",
    "attr_batch, preds_batch = exp_att.generate_attributions_batch(shape, model, device, dataloader_train_not_shuffled, baselines)\n",
    "attributions_train.update(attr_batch)\n",
    "predictions_train.update(preds_batch)\n",
    "\n",
    "del attr_batch\n",
    "del preds_batch\n",
    "\n",
    "attr_single, preds_single = exp_att.generate_attributions_single(shape, model, device, dataloader_train_not_shuffled, baselines)\n",
    "attributions_train.update(attr_single)\n",
    "predictions_train.update(preds_single)\n",
    "\n",
    "del attr_single\n",
    "del preds_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d54b33f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start with DeepLiftShap             :   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :  50%|█████     | 1/2 [03:20<03:20, 200.74s/it]\u001b[A\n",
      "Start with delete_to_mean 1         :  50%|█████     | 1/2 [03:20<03:20, 200.74s/it]\u001b[A\n",
      "Start with delete_to_mean 1         : 100%|██████████| 2/2 [06:33<00:00, 196.24s/it]\u001b[A\n",
      "Start with GradientShap             :  14%|█▍        | 1/7 [06:33<39:23, 393.86s/it]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :  50%|█████     | 1/2 [03:14<03:14, 194.72s/it]\u001b[A\n",
      "Start with delete_to_mean 1         :  50%|█████     | 1/2 [03:14<03:14, 194.72s/it]\u001b[A\n",
      "Start with delete_to_mean 1         : 100%|██████████| 2/2 [06:39<00:00, 200.52s/it]\u001b[A\n",
      "Start with IntegratedGradients      :  29%|██▊       | 2/7 [13:13<33:05, 397.08s/it]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :  50%|█████     | 1/2 [03:15<03:15, 195.99s/it]\u001b[A\n",
      "Start with delete_to_mean 1         :  50%|█████     | 1/2 [03:15<03:15, 195.99s/it]\u001b[A\n",
      "Start with delete_to_mean 1         : 100%|██████████| 2/2 [06:34<00:00, 197.19s/it]\u001b[A\n",
      "Start with Saliency                 :  43%|████▎     | 3/7 [19:47<26:22, 395.70s/it]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :  50%|█████     | 1/2 [03:35<03:35, 215.36s/it]\u001b[A\n",
      "Start with delete_to_mean 1         :  50%|█████     | 1/2 [03:35<03:35, 215.36s/it]\u001b[A\n",
      "Start with delete_to_mean 1         : 100%|██████████| 2/2 [07:03<00:00, 210.84s/it]\u001b[A\n",
      "Start with DeepLift                 :  57%|█████▋    | 4/7 [26:50<20:19, 406.50s/it]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :  50%|█████     | 1/2 [03:10<03:10, 190.12s/it]\u001b[A\n",
      "Start with delete_to_mean 1         :  50%|█████     | 1/2 [03:10<03:10, 190.12s/it]\u001b[A\n",
      "Start with delete_to_mean 1         : 100%|██████████| 2/2 [06:19<00:00, 189.43s/it]\u001b[A\n",
      "Start with Occlusion                :  71%|███████▏  | 5/7 [33:09<13:13, 396.62s/it]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :  50%|█████     | 1/2 [02:41<02:41, 161.68s/it]\u001b[A\n",
      "Start with delete_to_mean 1         :  50%|█████     | 1/2 [02:41<02:41, 161.68s/it]\u001b[A\n",
      "Start with delete_to_mean 1         : 100%|██████████| 2/2 [05:21<00:00, 160.74s/it]\u001b[A\n",
      "Start with KernelShap               :  86%|████████▌ | 6/7 [38:31<06:11, 371.17s/it]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Start with delete_to_zero 1         :  50%|█████     | 1/2 [03:33<03:33, 213.01s/it]\u001b[A\n",
      "Start with delete_to_mean 1         :  50%|█████     | 1/2 [03:33<03:33, 213.01s/it]\u001b[A\n",
      "Start with delete_to_mean 1         : 100%|██████████| 2/2 [07:05<00:00, 212.70s/it]\u001b[A\n",
      "                                                                                    \u001b[A\r"
     ]
    }
   ],
   "source": [
    "overall_results = exp_pa.perturbation_analysis(attributions_train, X_train, y_train, model, device, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_attr_name in attributions_train:\n",
    "    cur_attr_name = cur_attr_name.lower().replace(' ', '_')\n",
    "    cur_dir = f'{base_dir}/method-{cur_attr_name}'\n",
    "    for deletion_value in exp_pa.values:\n",
    "        deletion_value_fnc, deletion_length = deletion_value\n",
    "        name = deletion_value_fnc.__name__ + ' ' + str(deletion_length)\n",
    "        attribution = cur_attr_name\n",
    "        \n",
    "        results = overall_results[cur_attr_name][name]\n",
    "        exp_card.create_perturbation_analysis_card(dataset, attribution, name, X_train, results, cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b030cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.int64):\n",
    "            return str(obj)\n",
    "        if isinstance(obj, np.float32):\n",
    "            return str(obj)\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "overall_results_json = json.dumps(overall_results, cls=NumpyArrayEncoder)\n",
    "\n",
    "with open(f'{base_dir}/results.json', 'w') as f:\n",
    "    f.write(overall_results_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(f'./results/time-{cur_time}', 'zip', base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ecf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Captum",
   "language": "python",
   "name": "captum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
